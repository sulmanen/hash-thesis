@inproceedings{Alcantarilla2012,
abstract = {In this paper, we introduce KAZE features, a novelmultiscale 2D fea- ture detection and description algorithm in nonlinear scale spaces. Previous ap- proaches detect and describe features at different scale levels by building or ap- proximating the Gaussian scale space of an image. However, Gaussian blurring does not respect the natural boundaries of objects and smoothes to the same de- gree both details and noise, reducing localization accuracy and distinctiveness. In contrast, we detect and describe 2D features in a nonlinear scale space by means of nonlinear diffusion filtering. In thisway, we can make blurring locally adaptive to the image data, reducing noise but retaining object boundaries, obtaining su- perior localization accuracy and distinctiviness. The nonlinear scale space is built using efficient Additive Operator Splitting (AOS) techniques and variable con- ductance diffusion. We present an extensive evaluation on benchmark datasets and a practical matching application on deformable surfaces. Even though our features are somewhat more expensive to compute than SURF due to the con- struction of the nonlinear scale space, but comparable to SIFT, our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods.},
author = {Alcantarilla, Pablo Fern{\'{a}}ndez and Bartoli, Adrien and Davison, Andrew J.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33783-3_16},
isbn = {9783642337826},
issn = {03029743},
number = {PART 6},
pages = {214--227},
title = {{KAZE features}},
volume = {7577 LNCS},
year = {2012}
}
@article{Alcantarilla2013,
abstract = {We propose a novel and fast multiscale feature detection and description approach that exploits the benefits of nonlinear scale spaces. Previous attempts to detect and de- scribe features in nonlinear scale spaces are highly time consuming due to the compu- tational burden of creating the nonlinear scale space. In this paper we propose to use recent numerical schemes called Fast Explicit Diffusion (FED) embedded in a pyrami- dal framework to dramatically speed-up feature detection in nonlinear scale spaces. In addition, we introduce a Modified-Local Difference Binary (M-LDB) descriptor that is highly efficient, exploits gradient information from the nonlinear scale space, is scale and rotation invariant and has low storage requirements. We present an extensive evaluation that shows the excellent compromise between speed and performance of our approach compared to state-of-the-art methods such as BRISK, ORB, SURF, SIFT and KAZE.},
author = {Alcantarilla, Pablo and Nuevo, Jesus and Bartoli, Adrien},
doi = {10.5244/C.27.13},
isbn = {1-901725-49-9},
journal = {Procedings of the British Machine Vision Conference 2013},
pages = {13.1--13.11},
title = {{Fast Explicit Diffusion for Accelerated Features in Nonlinear Scale Spaces}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0013/index.html},
year = {2013}
}
@inproceedings{amato2013large,
author = {Amato, Giuseppe and Bolettieri, Paolo and Falchi, Fabrizio and Gennaro, Claudio},
booktitle = {International Conference on Similarity Search and Applications},
organization = {Springer},
pages = {245--256},
title = {{Large scale image retrieval using vector of locally aggregated descriptors}},
year = {2013}
}
@article{amato2016using,
author = {Amato, Giuseppe and Bolettieri, Paolo and Falchi, Fabrizio and Gennaro, Claudio and Vadicamo, Lucia},
journal = {arXiv preprint arXiv:1604.05576},
title = {{Using Apache Lucene to Search Vector of Locally Aggregated Descriptors}},
year = {2016}
}
@article{Andoni2008,
author = {ANDONI, Alexandr and INDYK, Piotr},
journal = {Communications of the ACM},
number = {1},
pages = {117--122},
publisher = {Association for Computing Machinery},
title = {{NEAR-OPTIMAL HASHING ALGORITHMS FOR APPROXIMATE NEAREST NEIGHBOR IN HIGH DIMENSIONS}},
volume = {51},
year = {2008}
}
@article{Andoni2015,
abstract = {We show the existence of a Locality-Sensitive Hashing (LSH) family for the angular distance that yields an approximate Near Neighbor Search algorithm with the asymptotically optimal running time exponent. Unlike earlier algorithms with this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn 2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving upon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also introduce a multiprobe version of this algorithm, and conduct experimental evaluation on real and synthetic data sets. We complement the above positive results with a fine-grained lower bound for the quality of any LSH family for angular distance. Our lower bound implies that the above LSH family exhibits a trade-off between evaluation time and quality that is close to optimal for a natural class of LSH functions.},
archivePrefix = {arXiv},
arxivId = {1509.02897},
author = {Andoni, Alexandr and Indyk, Piotr and Laarhoven, Thijs and Razenshteyn, Ilya and Schmidt, Ludwig},
eprint = {1509.02897},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Practical and Optimal LSH for Angular Distance}},
url = {http://arxiv.org/abs/1509.02897},
year = {2015}
}
@article{Arandjelovic2013,
abstract = {The objective of this paper is large scale object instance retrieval, given a query image. A starting point of such systems is feature detection and description, for example using SIFT. The focus of this paper, however, is towards very large scale retrieval where, due to storage requirements, very compact image descriptors are required and no information about the original SIFT descriptors can be accessed directly at run time. We start from VLAD, the state-of-the art compact descriptor introduced by Jegou et al. for this purpose, and make three novel contributions: first, we show that a simple change to the normalization method significantly improves retrieval performance, second, we show that vocabulary adaptation can substantially alleviate problems caused when images are added to the dataset after initial vocabulary learning. These two methods set a new state-of-the-art over all benchmarks investigated here for both mid-dimensional (20k-D to 30k-D) and small (128-D) descriptors. Our third contribution is a multiple spatial VLAD representation, MultiVLAD, that allows the retrieval and localization of objects that only extend over a small part of an image (again without requiring use of the original image SIFT descriptors).},
author = {Arandjelovic, R and Zisserman, A},
doi = {10.1109/CVPR.2013.207},
isbn = {1063-6919 VO -},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on},
keywords = {SIFT descriptor,feature extraction,image retrieval},
pages = {1578--1585},
title = {{All About VLAD}},
year = {2013}
}
@inproceedings{Bay2006,
abstract = {Abstract. In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Ro- bust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11744023_32},
isbn = {3540338322},
issn = {03029743},
pages = {404--417},
pmid = {16081019},
title = {{SURF: Speeded up robust features}},
volume = {3951 LNCS},
year = {2006}
}
@misc{Blei2008,
author = {Blei, Dave},
month = {apr},
publisher = {Princeton University Department of Computer Science},
title = {{COS 424: Interacting with Data}},
year = {2008}
}
@inproceedings{Broder1997,
abstract = {Given two documents A and B we define two mathematical notions: their resemblance r(A,B) and their containment c(A,B) that seem to capture well the informal notions of “roughly the same” and “roughly contained.” The basic idea is to reduce these issues to set intersection problems that can be easily evaluated by a process of random sampling that can be done inde- pendently for each document. Furthermore, the resemblance can be evaluated using a fixed size sample for each document. This paper discusses the mathematical properties of these measures and the efficient implementation of the sampling process using Rabin fingerprints.},
author = {Broder, Andrei Z},
booktitle = {Proceedings of the Compression and Complexity of Sequences 1997},
doi = {10.1109/SEQUEN.1997.666900},
isbn = {0-8186-8132-2},
issn = {0818681322},
pages = {21--29},
title = {{On the Resemblance and Containment of Documents}},
url = {http://dl.acm.org/citation.cfm?id=829502.830043},
year = {1997}
}
@article{Chum2010,
abstract = {We propose a randomized data mining method that finds clusters of spatially overlapping images. The core of the method relies on the min-Hash algorithm for fast detection of pairs of images with spatial overlap, the so-called cluster seeds. The seeds are then used as visual queries to obtain clusters which are formed as transitive closures of sets of partially overlapping images that include the seed. We show that the probability of finding a seed for an image cluster rapidly increases with the size of the cluster. The properties and performance of the algorithm are demonstrated on data sets with 10(4), 10(5), and 5 x 10(6) images. The speed of the method depends on the size of the database and the number of clusters. The first stage of seed generation is close to linear for databases sizes up to approximately 2(34) approximately 10(10) images. On a single 2.4 GHz PC, the clustering process took only 24 minutes for a standard database of more than 100,000 images, i.e., only 0.014 seconds per image.},
author = {Chum, Ondrej and Matas, Jiř{\'{i}}},
doi = {10.1109/TPAMI.2009.166},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Bag of words,Image clustering,Image retrieval,MinHash},
number = {2},
pages = {371--377},
pmid = {20075465},
title = {{Large-scale discovery of spatially related images}},
volume = {32},
year = {2010}
}
@inproceedings{Chum2009,
abstract = {We propose a novel hashing scheme for image retrieval, clustering and automatic object discovery. Unlike commonly used bag-of-words approaches, the spatial extent of image features is exploited in our method. The geometric information is used both to construct repeatable hash keys and to increase the discriminability of the description. Each hash key combines visual appearance (visual words) with semi-local geometric information. Compared with the state-of-the-art min-hash, the proposed method has both higher recall (probability of collision for hashes on the same object) and lower false positive rates (random collisions). The advantages of geometric min-hashing approach are most pronounced in the presence of viewpoint and scale change, significant occlusion or small physical overlap of the viewing fields. We demonstrate the power of the proposed method on small object discovery in a large unordered collection of images and on a large scale image clustering problem.},
author = {Chum, Ondrej and Perd{\&}apos;och, Michal and Matas, Jiri},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206531},
isbn = {9781424439935},
issn = {1063-6919},
pages = {17--24},
title = {{Geometric min-hashing: finding a (Thick) needle in a haystack}},
year = {2009}
}
@article{Chum2008,
abstract = {Image comparision on large data sets. we build on a min-Hash method 2, 4 that addresses (through a similarity threshold parameter) a whole range of near duplicate images: from images that appear, to a human observer, to be identical or very similar to images of the same scene or object.},
author = {Chum, Ondřej and Philbin, James and Zisserman, Andrew},
doi = {10.5244/C.22.50},
isbn = {1-901725-36-7},
issn = {10959203},
journal = {Proceedings of the British Machine Vision Conference},
pages = {4},
pmid = {17734329},
title = {{Near Duplicate Image Detection : min-Hash and tf-idf Weighting}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.142.2250{\&}rep=rep1{\&}type=pdf},
volume = {3},
year = {2008}
}
@article{Coskun2004,
author = {Coskun, B. and Sankur, B.},
journal = {IEEE Proceedings of the Signal Processing and Communications Applications Conference},
pages = {292--295},
title = {sc},
year = {2004}
}
@article{craven2005markov,
author = {Craven, Joseph Bockhorst Mark and Bockhorst, Joseph},
journal = {Advances in Neural Information Processing Systems},
pages = {193},
title = {{Markov networks for detecting overlapping elements in sequence data}},
volume = {17},
year = {2005}
}
@article{Davis2006,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
author = {Davis, Jesse and Goadrich, Mark},
doi = {10.1145/1143844.1143874},
isbn = {1595933832},
issn = {14710080},
journal = {Proceedings of the 23rd International Conference on Machine learning -- ICML'06},
pages = {233--240},
pmid = {19165215},
title = {{The Relationship Between Precision-Recall and ROC Curves}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874$\backslash$nhttp://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves},
year = {2006}
}
@article{Delhumeau2013,
abstract = {Recent works on image retrieval have proposed to index images by compact representations encoding powerful local descriptors, such as the closely related VLAD and Fisher vector. By combining such a representation with a suitable coding technique, it is possible to encode an image in a few dozen bytes while achieving excellent retrieval results. This paper revisits some assumptions proposed in this context regarding the handling of "visual burstiness", and shows that ad-hoc choices are implicitly done which are not desirable. Focusing on VLAD without loss of generality, we propose to modify several steps of the original design. Albeit simple, these modifications significantly improve VLAD and make it compare favorably against the state of the art.},
author = {Delhumeau, Jonathan and Gosselin, Philippe-Henri and J{\'{e}}gou, Herv{\'{e}} and P{\'{e}}rez, Patrick},
doi = {10.1145/2502081.2502171},
isbn = {9781450324045},
journal = {Proceedings of the 21st ACM international conference on Multimedia - MM '13},
keywords = {image search,multimedia retrieval,vlad},
pages = {653--656},
title = {{Revisiting the VLAD image representation}},
url = {http://dl.acm.org/citation.cfm?doid=2502081.2502171},
year = {2013}
}
@inproceedings{dong2012high,
author = {Dong, Wei and Wang, Zhe and Charikar, Moses and Li, Kai},
booktitle = {Proceedings of the 2nd ACM International Conference on Multimedia Retrieval},
organization = {ACM},
pages = {1},
title = {{High-confidence near-duplicate image detection}},
year = {2012}
}
@article{Fawcett2006,
abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research. ?? 2005 Elsevier B.V. All rights reserved.},
author = {Fawcett, Tom},
doi = {10.1016/j.patrec.2005.10.010},
isbn = {226},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classifier evaluation,Evaluation metrics,ROC analysis},
number = {8},
pages = {861--874},
pmid = {9820922},
title = {{An introduction to ROC analysis}},
volume = {27},
year = {2006}
}
@inproceedings{Fridrich1999,
abstract = {We describe an algorithm for robust extraction of bits from image blocks and a method for synthesizing a Gaussian pseudo-random sequence from those bits. The bits are extracted by thresholding projections onto random smooth patterns generated from a user-specified key. The extracted bits are further utilized to synthesize a Gaussian pseudo-random sequence that changes continuously with the image block yet depends sensitively on the secret key. The proposed technique is quite general and can be combined with the majority of oblivious watermarking schemes that generate watermarks from pseudo-random sequences. We anticipate that this algorithm will find applications in many oblivious watermarking schemes including secure data embedding into videos and watermarking images for tamper detection},
author = {Fridrich, J.},
booktitle = {Proceedings IEEE International Conference on Multimedia Computing and Systems},
doi = {10.1109/MMCS.1999.778542},
isbn = {0-7695-0253-9},
keywords = {Copyright protection,Cryptography,Digital images,Fingerprint recognition,Gaussian pseudo-random sequence,Image storage,Intelligent systems,Public key,Robustness,Videos,Watermarking,copyright,image blocks,image segmentation,image sequences,image thresholding,random smooth patterns,robust bit extraction,secret key,security of data,tamper detection,user-specified key,videos,watermarking},
pages = {536--540},
title = {{Robust bit extraction from images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=778542},
volume = {2},
year = {1999}
}
@article{Gionis1999,
abstract = {The nearest- or near-neighbor query problems arise in a large variety of database applications, usually in the context of similarity searching. Of late, there has been increasing interest in building search/index structures for performing similarity search over high-dimensional data, e.g., image databases, document collections, time-series databases, and genome databases. Unfortunately, all known techniques for solving this problem fall prey to the "curse of dimensionality." That is, the data...},
author = {Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev},
doi = {10.1.1.41.4809},
isbn = {1-55860-615-7},
issn = {08941912},
journal = {VLDB '99 Proceedings of the 25th International Conference on Very Large Data Bases},
number = {1},
pages = {518--529},
pmid = {276702},
title = {{Similarity Search in High Dimensions via Hashing}},
url = {http://www.cs.princeton.edu/courses/archive/spring13/cos598C/Gionis.pdf$\backslash$nhttp://portal.acm.org/citation.cfm?id=671516},
volume = {99},
year = {1999}
}
@book{Gonzalez2002,
abstract = {Digital Image Processing has been the world-wide leading textbook in its field for almost 30 years. As the 1977 and 1987 editions by Gonzalez and Wintz, and the 1992 edition by Gonzalez and Woods, the present edition was prepared with students and instructors in mind. The material is timely, highly readable, and illustrated with numerous examples of practical significance. All mainstream areas of image processing are covered, including a totally revised introduction and discussion of image fundamentals, image enhancement in the spatial and frequency domains, restoration, color image processing, wavelets, image compression, morphology, segmentation, and image description. Coverage concludes with a discussion on the fundamentals of object recognition. Read more at http://ebookee.org/Digital-Image-Processing-2Ed-Gonzalez-woods{\_}59369.html{\#}x6zHhXYRriyOVDcM.99},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gonzalez, Rc and Woods, Re},
booktitle = {Prentice Hall},
doi = {10.1016/0734-189X(90)90171-Q},
eprint = {arXiv:1011.1669v3},
isbn = {0201180758},
issn = {0024094X},
pages = {190},
pmid = {21468981},
title = {{Digital image processing}},
url = {http://mirror.klaus-uwe.me/ctan/biblio/bibtex/contrib/persian-bib/Persian-bib-userguide.pdf$\backslash$nhttp://ftp.neu6.edu.cn/mirrors/CTAN/biblio/bibtex/contrib/persian-bib/Persian-bib-userguide.pdf},
year = {2002}
}
@inproceedings{Grewenig2010,
abstract = {There are two popular ways to implement anisotropic diffusion filters with a diffusion tensor: Explicit finite difference schemes are simple but become inefficient due to severe time step size restrictions, while semi-implicit schemes are more efficient but require to solve large linear systems of equations. In our paper we present a novel class of algorithms that combine the advantages of both worlds: They are based on simple explicit schemes, while being more efficient than semi-implicit approaches. These so-called fast explicit diffusion (FED) schemes perform cycles of explicit schemes with varying time step sizes that may violate the stability restriction in up to 50 percent of all cases. FED schemes can be motivated from a decomposition of box filters in terms of explicit schemes for linear diffusion problems. Experiments demonstrate the advantages of the FED approach for time-dependent (parabolic) image enhancement problems as well as for steady state (elliptic) image compression tasks. In the latter case FED schemes are speeded up substantially by embedding them in a cascadic coarse-to-fine approach.},
author = {Grewenig, Sven and Weickert, Joachim and Bruhn, Andr{\'{e}}s},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15986-2_54},
isbn = {3642159850},
issn = {03029743},
pages = {533--542},
title = {{From box filtering to fast explicit diffusion}},
volume = {6376 LNCS},
year = {2010}
}
@incollection{Hadmi2012,
author = {Hadmi, Azhar and Puech, William and {Ait Es Said}, Brahim and {Ait Ouahman}, Abdellah},
booktitle = {Watermarking - Volume 2},
pages = {17--42},
title = {{Perceptual Image Hashing}},
url = {http://www.intechopen.com/books/watermarking-volume-2/perceptual-image-hashing},
year = {2012}
}
@article{Hamming1950,
abstract = {Hamming, Richard W. (1950), "Error detecting and error correcting codes" (PDF), Bell System Technical Journal 29 (2): 147–160, doi:10.1002/j.1538-7305.1950.tb00463.x, MR 0035935.},
author = {Hamming, R. W.},
doi = {10.1002/j.1538-7305.1950.tb00463.x},
isbn = {0005-8580},
issn = {15387305},
journal = {Bell System Technical Journal},
number = {2},
pages = {147--160},
title = {{Error Detecting and Error Correcting Codes}},
volume = {29},
year = {1950}
}
@article{Jaakkola1999,
abstract = {Generative probability models such as hidden Markov models provide$\backslash$na principled way of treating missing information and dealing with$\backslash$nvariable length sequences. On the other hand, discriminative $\backslash$n$\backslash$nmethods such as support vector machines enable us to construct flexible$\backslash$ndecision boundaries and often result in classification performance$\backslash$nsuperior to that of the model based approaches. An ideal classifier$\backslash$nshould combine these two complementary approaches. In this paper,$\backslash$nwe develop a natural way of achieving this combination by deriving$\backslash$nkernel functions for use in discriminative methods such as support$\backslash$nvector machines from generative probability models. We provide a$\backslash$ntheoretical justication for this combination as well as demonstrate$\backslash$na substantial improvement in the classifcation performance in the$\backslash$ncontext of DNA and protein sequence analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jaakkola, T.S. and Haussler, D.},
doi = {10.1038/217994a0},
eprint = {arXiv:1011.1669v3},
isbn = {0262112450},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {487--493},
pmid = {15003161},
title = {{Exploiting generative models in discriminative classifiers}},
year = {1999}
}
@misc{Jegou2014,
author = {J{\'{e}}gou, Herv{\'{e}}},
month = {jun},
publisher = {IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Large-scale visual recognition, I - Efficient matching and indexing}},
year = {2014}
}
@article{Jegou2008,
abstract = {This technical report presents and extends a recent paper we have proposed for large scale image search. State-of-the-art methods build on the bag-of- features image representation. We first analyze bag-of-features in the framework of approximate nearest neighbor search. This shows the sub-optimality of such a representation for matching descriptors and leads us to derive a more precise representation based on 1) Hamming embedding (HE) and 2) weak geometric consistency constraints (WGC). HE provides binary signatures that refine the matching based on visual words. WGC filters matching descriptors that are not consistent in terms of angle and scale. HE and WGC are integrated within an inverted file and are efficiently exploited for all images, even in the case of very large datasets. Experiments performed on a dataset of one million of images show a significant improvement due to the binary signature and the weak geometric consistency constraints, as well as their efficiency. Estimation of the full geometric transformation, i.e., a re-ranking step on a short list of images, is complementary to our weak geometric consistency constraints and allows to further improve the accuracy.},
author = {Jegou, Herve and Douze, Matthijs and Schmid, Cordelia},
doi = {10.1007/978-3-540-88682-2_24},
isbn = {9783540886815},
issn = {03029743},
journal = {Eccv},
number = {October},
pages = {304--317},
title = {{Hamming Embedding and Weak Geometry Consistency for Large Scale Image Search – Extended version –}},
year = {2008}
}
@inproceedings{Jegou2009,
abstract = {Burstiness, a phenomenon initially observed in text retrieval, is the property that a given visual element appears more times in an image than a statistically independent model would predict. In the context of image search, burstiness corrupts the visual similarity measure, i.e., the scores used to rank the images. In this paper, we propose a strategy to handle visual bursts for bag-of-features based image search systems. Experimental results on three reference datasets show that our method significantly and consistently outperforms the state of the art.},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206609},
isbn = {9781424439935},
issn = {1063-6919},
pages = {1169--1176},
title = {{On the burstiness of visual elements}},
year = {2009}
}
@article{Jegou2011,
abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia},
doi = {10.1109/TPAMI.2010.57},
eprint = {arXiv:1011.1669v3},
isbn = {9781450300551},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {High-dimensional indexing,approximate search,image indexing,very large databases},
number = {1},
pages = {117--128},
pmid = {21088323},
title = {{Product quantization for nearest neighbor search}},
volume = {33},
year = {2011}
}
@inproceedings{Jegou2010,
abstract = {We address the problem of image search on a very large scale, where three constraints have to be considered jointly: the accuracy of the search, its efficiency, and the memory usage of the representation. We first propose a simple yet efficient way of aggregating local image descriptors into a vector of limited dimension, which can be viewed as a simplification of the Fisher kernel representation. We then show how to jointly optimize the dimension reduction and the indexing algorithm, so that it best preserves the quality of vector comparison. The evaluation shows that our approach significantly outperforms the state of the art: the search accuracy is comparable to the bag-of-features approach for an image representation that fits in 20 bytes. Searching a 10 million image dataset takes about 50ms.},
author = {J{\'{e}}gou, Herv{\'{e}} and Douze, Matthijs and Schmid, Cordelia and P{\'{e}}rez, Patrick},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540039},
isbn = {9781424469840},
issn = {10636919},
pages = {3304--3311},
pmid = {22156101},
title = {{Aggregating local descriptors into a compact image representation}},
year = {2010}
}
@article{Jegou2012,
abstract = {This paper addresses the problem of large-scale image search. Three constraints have to be taken into account: search accuracy, efficiency, and memory usage. We first present and evaluate different ways of aggregating local image descriptors into a vector and show that the Fisher kernel achieves better performance than the reference bag-of-visual words approach for any given vector dimension. We then jointly optimize dimensionality reduction and indexing in order to obtain a precise vector comparison as well as a compact representation. The evaluation shows that the image representation can be reduced to a few dozen bytes while preserving high accuracy. Searching a 100 million image data set takes about 250 ms on one processor core.},
author = {J{\'{e}}gou, Herv{\'{e}} and Perronnin, Florent and Douze, Matthijs and S??nchez, Jorge and P??rez, Patrick and Schmid, Cordelia},
doi = {10.1109/TPAMI.2011.235},
isbn = {0162-8828 VO - 34},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Image search,image retrieval,indexing},
number = {9},
pages = {1704--1716},
pmid = {22156101},
title = {{Aggregating local image descriptors into compact codes}},
volume = {34},
year = {2012}
}
@incollection{mihccak2001new,
author = {Kıvan, M},
booktitle = {Security and privacy in digital rights management},
pages = {13--21},
publisher = {Springer},
title = {{New Iterative Geometric Methods for Robust Perceptual Image Hashing}},
year = {2002}
}
@inproceedings{Lee2010,
abstract = {In this paper, we propose Partition min-Hash (PmH), a novel hashing scheme for discovering partial duplicate images from a large database. Unlike the standard min-Hash algorithm that assumes a bag of words image representa- tion, our approach utilizes the fact that duplicate regions among images are often localized. By theoretical analysis, simulation, and empirical study, we show that PmHoutperforms standard min-Hash in terms of precision and recall, while being orders of magnitude faster. When combined with the start-of-the-art Geometric min-Hash algorithm, our approach speeds up hashing by 10 times without losing precision or recall. When given a fixed time budget, our method achieves much higher recall than the state-of-the-art.},
author = {Lee, David C. and Ke, Qifa and Isard, Michael},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15549-9_47},
isbn = {3642155480},
issn = {03029743},
keywords = {min-hash,partial duplicate image discovery,partition min-hash},
number = {PART 1},
pages = {648--662},
title = {{Partition min-hash for partial duplicate image discovery}},
volume = {6311 LNCS},
year = {2010}
}
@article{Li2015,
abstract = {In this paper, we study the problem of discovering visual patteLi, W., Wang, C., Zhang, L., Rui, Y., {\&} Zhang, B. (2015). Partial-Duplicate Clustering and Visual Pattern Discovery on Web Scale Image Database. IEEE Transactions on Multimedia, 17(7), 967–980. http://doi.org/10.1109/TMM.2015.2428996rns and partial-duplicate images, which is fundamental to visual concept representation and image parsing, but very challenging when the database is extremely large, such as billions of images indexed by a commercial search engine. Although extensive research with sophisticated algorithms has been conducted for either partial-duplicate clustering or visual pattern discovery, most of them can not be easily extended to this scale, since both are clustering problems in nature and require pairwise comparisons. To tackle this computational challenge, we introduce a novel and highly parallelizable framework to discover partial-duplicate images and visual patterns in a unified way in distributed computing systems. We emphasize the nested property of local features, and propose the generalized nested feature (GNF) as a mid-level representation for regions and local patterns. Initial coarse clusters are then discovered by GNFs, upon which n-gram GNF is defined to represent co-occurrent visual patterns. After that, efficient merging and refining algorithms are used to get the partial-duplicate clusters, and logical combinations of probabilistic GNF models are leveraged to represent the visual patterns of partially duplicate images. Extensive experiments show the parallelizable property and effectiveness of the algorithms on both partial-duplicate clustering and visual pattern discovery. With 2000 machines, it costs about eight and 400 minutes to process one million and 40 million images respectively, which is quite efficient compared to previous methods.},
author = {Li, Wei and Wang, Changhu and Zhang, Lei and Rui, Yong and Zhang, Bo},
doi = {10.1109/TMM.2015.2428996},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Local features,parallel algorithms,partial - duplicate images,visual patterns},
number = {7},
pages = {967--980},
title = {{Partial-Duplicate Clustering and Visual Pattern Discovery on Web Scale Image Database}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84933520245{\&}partnerID=tZOtx3y1},
volume = {17},
year = {2015}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
eprint = {0112017},
isbn = {1568811012},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Image matching,Invariant features,Object recognition,Scale invariance},
number = {2},
pages = {91--110},
pmid = {20064111},
primaryClass = {cs},
title = {{Distinctive image features from scale-invariant keypoints}},
volume = {60},
year = {2004}
}
@inproceedings{Lowe1999,
abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low residual least squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially occluded images with a computation time of under 2 seconds},
archivePrefix = {arXiv},
arxivId = {cs/0112017},
author = {Lowe, David G.},
booktitle = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.1999.790410},
eprint = {0112017},
isbn = {0-7695-0164-8},
issn = {0-7695-0164-8},
number = {[8},
pages = {1150--1157},
pmid = {15806121},
primaryClass = {cs},
title = {{Object recognition from local scale-invariant features}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=790410},
volume = {2},
year = {1999}
}
@inproceedings{Miller1968,
abstract = {The literature concerning man-computer transactions abounds in controversy about the limits of "system response time" to a user's command or inquiry at a terminal. Two major semantic issues prohibit resolving this controversy. One issue centers around the question of "Response time to what?" The implication is that different human purposes and actions will have different acceptable or useful response times.},
author = {Miller, R.B.},
booktitle = {AFIPS'68 Proceedings of the December 9-11, 1968, fall joint computer conference, part I},
doi = {10.1145/1476589.1476628},
pages = {267--277},
title = {{Response time in man-computer conversational transactions}},
url = {http://portal.acm.org/citation.cfm?id=1476628},
year = {1968}
}
@article{Myers1985,
abstract = {A “percent-done progress indicator” is a graphical technique which allows the user to monitor the progress through the processing of a task. Progress indicators can be displayed on almost all types of output devices, and can be used with many different kinds of programs. Practical experience and formal experiments show that prograss indicators are an important and useful user-interface tool, and that they enhance the attractiveness and effectiveness of programs that incorporate them. This paper discusses why progress indicators are important. It includes the results of a formal experiment with progress indicators. One part of the experiment demonstrates that people prefer to have progress indicators. Another part attempted to replicate earlier findings to show that people prefer constant to variable response time in general, and then to show that this effect is reversed with progress indicators, but the results were not statistically significant. In fact, no significant preference for constant response time was shown, contrary to previously published results.},
author = {Myers, Brad a.},
doi = {10.1145/1165385.317459},
isbn = {0-89791-149-0},
issn = {07366906},
journal = {ACM SIGCHI Bulletin},
number = {4},
pages = {11--17},
pmid = {21915513},
title = {{The importance of percent-done progress indicators for computer-human interfaces}},
volume = {16},
year = {1985}
}
@article{Nielsen1993a,
abstract = {Summary: There are 3 main time limits (which are determined by human perceptual abilities) to keep in mind when optimizing web and application performance.},
author = {Nielsen, Jakob},
journal = {Usability Engineering},
title = {{Response times: The 3 important limits}},
year = {1993}
}
@book{Nielsen1993,
abstract = {"The purpose of Jakob Nielsen's Usability Engineering is to help nontechnical people improve the systems so thatthey are not only error-free but also easier and more pleasant to use, and more efficient. It is a book that ...shows ushow to change the world and does so admirably....One of this book's strengths is that it provides a wide selection ofmethods for improving systems, and allows for the unavoidable constraints of the real world." -NEW SCIENTIST Written by the author of the best-selling HyperText {\&} HyperMedia, this book is an excellent guide to the methodsof usability engineering. The book provides the tools needed to avoid usability surprises and improve product quality.Step-by-step information on which method to use at various stages during the development lifecycle are included,along with detailed information on how to run a usability test and the unique issues relating to international usability. KEY FEATURESEmphasizes cost-effective methods that developers can implement immediately.Instructs readers about which methods to use and when to use them throughout the development lifecycle, ultimately helping in cost-benefit analysis. Shows readers how to avoid the four most frequently listed reasons for delay in software projects. Includes detailed information on how to run a usability test. Covers unique issues of international usability. Features an extensive bibliography allowing readers to find additional information. Written by an internationally renowned expert in the field and the author of the best-selling HyperText {\&} HyperMedia.},
author = {Nielsen, Jakob},
booktitle = {Usability Engineering},
doi = {10.1145/1508044.1508050},
isbn = {0125184069},
issn = {10772626},
number = {3},
pages = {362},
pmid = {18369261},
title = {{Usability Engineering}},
url = {http://www.useit.com/jakob/useengbook.html},
volume = {44},
year = {1993}
}
@article{Nielsen2010,
abstract = {From 0.1 seconds to 10 years or more, user interface design has many different timeframes, and each has its own particular usability issues.},
author = {Nielsen, Jakob},
journal = {Time},
pages = {10--15},
title = {{Powers of 10 : Time Scales in User Experience}},
year = {2010}
}
@article{Perona1990,
abstract = {A new definition of scale-space is suggested, and a class of algorithms used to realize a diffusion process is introduced. The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing rather than interregion smoothing. It is shown that the `no new maxima should be generated at coarse scales' property of conventional scale space is preserved. As the region boundaries in the approach remain sharp, a high-quality edge detector which successfully exploits global information is obtained. Experimental results are shown on a number of images. Parallel hardware implementations are made feasible because the algorithm involves elementary, local operations replicated over the image},
author = {Perona, P and Malik, J},
doi = {10.1109/34.56205},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Anisotropic magnetoresistance,Detectors,Diffusion processes,Equations,Filtering,Hardware,Image edge detection,Image generation,Image resolution,Kernel,Smoothing methods,anisotropic diffusion,edge detection,filtering and prediction theory,intraregion smoothing,parallel processing,pattern recognition,picture processing,scale-space},
number = {7},
pages = {629--639},
pmid = {19268610},
title = {{Scale-space and edge detection using anisotropic diffusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=56205},
volume = {12},
year = {1990}
}
@inproceedings{Perronnin2007,
abstract = {Within the field of pattern classification, the Fisher kernel is a powerful framework which combines the strengths of generative and discriminative approaches. The idea is to characterize a signal with a gradient vector derived from a generative probability model and to subsequently feed this representation to a discriminative classifier. We propose to apply this framework to image categorization where the input signals are images and where the underlying generative model is a visual vocabulary: a Gaussian mixture model which approximates the distribution of low-level features in images. We show that Fisher kernels can actually be understood as an extension of the popular bag-of-visterms. Our approach demonstrates excellent performance on two challenging databases: an in-house database of 19 object/scene categories and the recently released VOC 2006 database. It is also very practical: it has low computational needs both at training and test time and vocabularies trained on one set of categories can be applied to another set without any significant loss in performance.},
author = {Perronnin, Florent and Dance, Christopher},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383266},
isbn = {1424411807},
issn = {10636919},
title = {{Fisher kernels on visual vocabularies for image categorization}},
year = {2007}
}
@inproceedings{Perronnin2010a,
abstract = {The problem of large-scale image search has been traditionally addressed with the bag-of-visual-words (BOV). In this article, we propose to use as an alternative the Fisher kernel framework. We first show why the Fisher representation is well-suited to the retrieval problem: it describes an image by what makes it different from other images. One drawback of the Fisher vector is that it is high-dimensional and, as opposed to the BOV, it is dense. The resulting memory and computational costs do not make Fisher vectors directly amenable to large-scale retrieval. Therefore, we compress Fisher vectors to reduce their memory footprint and speed-up the retrieval. We compare three binarization approaches: a simple approach devised for this representation and two standard compression techniques. We show on two publicly available datasets that compressed Fisher vectors perform very well using as little as a few hundreds of bits per image, and significantly better than a very recent compressed BOV approach.},
author = {Perronnin, Florent and Liu, Yan and S{\'{a}}nchez, Jorge and Poirier, Herv{\'{e}}},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540009},
isbn = {9781424469840},
issn = {10636919},
pages = {3384--3391},
title = {{Large-scale image retrieval with compressed fisher vectors}},
year = {2010}
}
@inproceedings{Perronnin2010,
abstract = {The Fisher kernel (FK) is a generic framework which com-bines the benefits of generative and discriminative approaches. In the context of image classification the FK was shown to extend the popular bag-of-visual-words (BOV) by going beyond count statistics. However, in practice, this enriched representation has not yet shown its superiority over the BOV. In the first part we show that with several well-motivated modifications over the original framework we can boost the accuracy of the FK. On PASCAL VOC 2007 we increase the Average Precision (AP) from 47.9{\%} to 58.3{\%}. Similarly, we demonstrate state-of-the-art accuracy on CalTech 256. A major advantage is that these results are obtained us-ing only SIFT descriptors and costless linear classifiers. Equipped with this representation, we can now explore image classification on a larger scale. In the second part, as an application, we compare two abundant re-sources of labeled images to learn classifiers: ImageNet and Flickr groups. In an evaluation involving hundreds of thousands of training images we show that classifiers learned on Flickr groups perform surprisingly well (although they were not intended for this purpose) and that they can complement classifiers learned on more carefully annotated datasets.},
author = {Perronnin, Florent and S{\'{a}}nchez, Jorge and Mensink, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15561-1_11},
isbn = {364215560X},
issn = {03029743},
number = {PART 4},
pages = {143--156},
title = {{Improving the Fisher kernel for large-scale image classification}},
volume = {6314 LNCS},
year = {2010}
}
@misc{PhilbinJamesArandjelovicReljaZisserman2012,
author = {{Philbin, James Arandjelovi{\'{c}}, Relja Zisserman}, Andrew},
title = {{The Oxford Buildings Dataset}},
url = {http://www.robots.ox.ac.uk/{~}vgg/data/oxbuildings/},
year = {2012}
}
@inproceedings{Philbin2007,
abstract = {In this paper, we present a large-scale object retrieval system. The user supplies a query object by selecting a region of a query image, and the system returns a ranked list of images that contain the same object, retrieved from a large corpus. We demonstrate the scalability and performance of our system on a dataset of over 1 million images crawled from the photo-sharing site, Flickr [3], using Oxford landmarks as queries. Building an image-feature vocabulary is a major time and performance bottleneck, due to the size of our dataset. To address this problem we compare different scalable methods for building a vocabulary and introduce a novel quantization method based on randomized trees which we show outperforms the current state-of-the-art on an extensive ground-truth. Our experiments show that the quantization has a major effect on retrieval quality. To further improve query performance, we add an efficient spatial verification stage to re-rank the results returned from our bag-of-words model and show that this consistently improves search quality, though by less of a margin when the visual vocabulary is large. We view this work as a promising step towards much larger, "web-scale " image corpora.},
author = {Philbin, James and Chum, Ondřej and Isard, Michael and Sivic, Josef and Zisserman, Andrew},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383172},
isbn = {1424411807},
issn = {10636919},
title = {{Object retrieval with large vocabularies and fast spatial matching}},
year = {2007}
}
@article{Provost1997,
abstract = {We analyze critically the use of classification accuracy to compare classifiers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classifiers and drawinto question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scientific conclusions.},
author = {Provost, Foster and Fawcett, Tom and Kohavi, Ron},
isbn = {1-55860-556-8},
journal = {Proceedings of the Fifteenth International Conference on Machine Learning},
pages = {445--453},
pmid = {15933812881907238759},
title = {{The Case Against Accuracy Estimation for Comparing Induction Algorithms}},
year = {1997}
}
@article{Sivic2003,
abstract = {We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieved is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching in two full length feature films.},
archivePrefix = {arXiv},
arxivId = {1504.06897},
author = {Sivic, J. and Zisserman, A.},
doi = {10.1109/ICCV.2003.1238663},
eprint = {1504.06897},
isbn = {0-7695-1950-4},
issn = {00189219},
journal = {IEEE International Conference on Computer Vision},
pages = {1470--1477},
title = {{Video Google: a text retrieval approach to object matching in videos}},
url = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=1238663},
year = {2003}
}
@misc{Vedaldi2012,
author = {Vedaldi, Andrea and Zisserman, Andrew},
title = {{Recognition of object instaces practical}},
url = {http://www.robots.ox.ac.uk/{~}vgg/practicals/instance-recognition/index.html},
urldate = {2016-04-30},
year = {2012}
}
@article{Viola2001,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
author = {Viola, P and Jones, M},
doi = {10.1109/CVPR.2001.990517},
isbn = {0-7695-1272-0},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition (CVPR)},
keywords = {AdaBoost,Detectors,Face detection,Filters,Focusing,Image representation,Machine learning,Object detection,Pixel,Robustness,Skin,background regions,boosted simple feature cascade,classifiers,face detection,feature extraction,image classification,image processing,image representation,integral image,learning (artificial intelligence),machine learning,object detection,object specific focus-of-attention mechanism,rapid object detection,real-time applications,statistical guarantees,visual object detection},
pages = {I----511----I----518},
pmid = {7143246},
title = {{Rapid object detection using a boosted cascade of simple features}},
volume = {1},
year = {2001}
}
@inproceedings{Wang2013,
abstract = {Duplicate image discovery, or discovering duplicate image clusters, is a challenging problem for billions of Internet images due to the lack of good distance metric which both covers the large variation within a duplicate image cluster and eliminates false alarms. After carefully investigating existing local and global features that have been widely used for large-scale image search and indexing, we propose a two-step approach that combines both local and global features: global descriptors are used to discover seed clusters with high precision, whereas local descriptors are used to grow the seeds to cover good recall. Using efficient hashing techniques for both features and the MapReduce framework, our system is able to discover about 553.8 million duplicate images from 2 billion Internet images within 13 hours on a 2, 000 core cluster.},
author = {Wang, Xin Jing and Zhang, Lei and Liu, Ce},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2013.71},
isbn = {9780769549903},
issn = {21607508},
pages = {429--436},
title = {{Duplicate discovery on 2 billion internet images}},
year = {2013}
}
@inproceedings{Wang2013a,
abstract = {Duplicate image discovery, or discovering duplicate image clusters, is a challenging problem for billions of Internet images due to the lack of good distance metric which both covers the large variation within a duplicate image cluster and eliminates false alarms. After carefully investigating existing local and global features that have been widely used for large-scale image search and indexing, we propose a two-step approach that combines both local and global features: global descriptors are used to discover seed clusters with high precision, whereas local descriptors are used to grow the seeds to cover good recall. Using efficient hashing techniques for both features and the MapReduce framework, our system is able to discover about 553.8 million duplicate images from 2 billion Internet images within 13 hours on a 2, 000 core cluster.},
author = {Wang, Xin Jing and Zhang, Lei and Liu, Ce},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2013.71},
isbn = {9780769549903},
issn = {21607508},
pages = {429--436},
title = {{Duplicate discovery on 2 billion internet images}},
year = {2013}
}
@article{Weickert1998,
abstract = {Nonlinear diffusion filtering in image processing is usually performed with explicit schemes. They are only stable for very small time steps, which leads to poor efficiency and limits their practical use. Based on a discrete nonlinear diffusion scale-space framework we present semi-implicit schemes which are stable for all time steps. These novel schemes use an additive operator splitting (AOS), which guarantees equal treatment of all coordinate axes. They can be implemented easily in arbitrary dimensions, have good rotational invariance and reveal a computational complexity and memory requirement which is linear in the number of pixels. Examples demonstrate that, under typical accuracy requirements, AOS schemes are at least ten times more efficient than the widely used explicit schemes.},
author = {Weickert, Joachim and {Ter Haar Romeny}, Bart M. and Viergever, Max A.},
doi = {10.1109/83.661190},
isbn = {1057-7149 VO - 7},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Absolute stability,Nonlinear diffusion,Recursive filters},
number = {3},
pages = {398--410},
pmid = {18276260},
title = {{Efficient and reliable schemes for nonlinear diffusion filtering}},
volume = {7},
year = {1998}
}
@inproceedings{Yang2006,
abstract = {Image perceptual hashing has been proposed to identify or authenticate image contents in a robust way against distortions caused by compression, noise, common signal processing and geometrical modifications, while still holding a good discriminability for different ones in sense of human perception. We propose and compare four normalized block mean value based image perceptual hashing algorithms which demonstrate higher performances than other existing algorithms in robustness-anddiscriminalibility and simplicity for implementation. Overlapped blocking and rotation operations are employed to enhance the robustness to geometrical distortions. To evaluate the proposed algorithms{\^{A}}¿ robustness and discriminability, given fixed modifications, identification ratio is used; and given fixed content classification, receiver operating curves is obtained.},
author = {Yang, Bian and Gu, Fan and Niu, Xiamu},
booktitle = {Proceedings - 2006 International Conference on Intelligent Information Hiding and Multimedia Signal Processing, IIH-MSP 2006},
doi = {10.1109/IIH-MSP.2006.265125},
isbn = {0769527450},
pages = {167--170},
title = {{Block mean value based image perceptual hashing}},
year = {2006}
}
@article{Zauner2010,
abstract = {Perceptual image hash functions produce hash values based on the image's visual appearance. A perceptual hash can also be referred to as e.g. a robust hash or a ngerprint. Such a function calculates similar hash values for similar images, whereas for dissimilar images dissimilar hash values are calculated. Finally, using an adequate distance or similarity function to compare two perceptual hash values, it can be decided whether two images are perceptually di erent or not. Perceptual image hash functions can be used e.g. for the identi cation or integrity veri cation of images. This thesis proposes a novel benchmarking framework, called Rihamark, for perceptual image hash functions. Subsequently, four di erent percep- tual image hash functions were benchmarked: A discrete Cosine transform (DCT) based , a Marr-Hildreth operator based, a radial variance based and a block mean value based image hash function. pHash, an open source im- plementation of various perceptual hash functions, was used to benchmark the rst three functions. The latter, the block mean value based image hash function was implemented by the author of this thesis himself. The block mean value based image hash function outperforms the other hash functions in terms of speed. The DCT based image hash function is the slowest. Although the Marr-Hildreth operator based image hash function is not the fastest nor the most robust, it o ers by far the best discriminiative abilities. Interestingly enough, the performance in terms of discriminiative ability does not depend on the content of the images. That is, no matter whether the visual appearance of the images compared was very similar or not, the performance of the particular hash function did not change sig- ni cantly. Di erent image operations, like horizontal ipping, rotating or resizing, were used to test the robustness of the image hash functions. An interesting result is that none of the tested image hash function is robust against ipping an image horizontally.},
author = {Zauner, Christoph},
journal = {Master's thesis, Upper Austria University of Applied {\ldots}},
pages = {107},
title = {{Implementation and benchmarking of perceptual image hash functions}},
url = {w},
year = {2010}
}
@misc{,
booktitle = {W3C Architecture Domain},
title = {{Naming and Addressing: URIs, URLs, ...}},
url = {https://www.w3.org/Addressing/},
urldate = {2016-04-12}
}
@misc{,
booktitle = {W3C Wiki},
title = {{Definition of User Agent}},
url = {https://www.w3.org/WAI/UA/work/wiki/Definition{\_}of{\_}User{\_}Agent},
urldate = {2016-04-12}
}
@article{fisher1922interpretation,
author = {Fisher, Ronald A},
journal = {Journal of the Royal Statistical Society},
number = {1},
pages = {87--94},
publisher = {JSTOR},
title = {{On the interpretation of $\chi^2$ from contingency tables, and the calculation of P}},
volume = {85},
year = {1922}
}
